{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TM10007_resit.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JSSchouten/TM10007_Group_10/blob/master/TM10007_resit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AV8PjjK_vMDk",
        "colab_type": "code",
        "outputId": "548131af-87c7-4fda-fcff-a43ad577a123",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Run this to use from colab environment\n",
        "!pip install -q --upgrade git+https://github.com/karinvangarderen/tm10007_project.git"
      ],
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for brats (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFZRWeciabPz",
        "colab_type": "text"
      },
      "source": [
        "# Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCMSf2jf7Rut",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "all necessary imports\n",
        "'''\n",
        "# general functions\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "# load data\n",
        "from brats.load_data  import load_data\n",
        "\n",
        "# preprocessing and scaling\n",
        "from sklearn.model_selection    import train_test_split\n",
        "from sklearn                    import preprocessing\n",
        "from sklearn.decomposition      import PCA\n",
        "from sklearn.feature_selection  import SelectKBest, f_classif\n",
        "\n",
        "# classifiers\n",
        "from sklearn.model_selection        import cross_val_score, learning_curve\n",
        "from sklearn.neighbors              import KNeighborsClassifier\n",
        "from sklearn.ensemble               import RandomForestClassifier\n",
        "from sklearn                        import svm\n",
        "\n",
        "# calculate accuracy values\n",
        "from sklearn.metrics  import accuracy_score\n",
        "from sklearn.metrics  import confusion_matrix\n",
        "from sklearn.metrics  import roc_auc_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eq2zJcVgvCGE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Load the data from GitHub\n",
        "'''\n",
        "data = load_data()\n",
        "data_columns = list(set(data))\n",
        "# print(f'The number of samples: {len(data.index)}')\n",
        "# print(f'The number of columns: {len(data.columns)}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuaIFrUQQTxA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "General functions\n",
        "'''\n",
        "\n",
        "\n",
        "def split(data):\n",
        "  '''\n",
        "  Divide data in a training and test set 80% - 20%\n",
        "  '''\n",
        "  x = data.iloc[:,:-1]\n",
        "  y = data['label']\n",
        "  x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
        "\n",
        "  return x_train, x_test, y_train, y_test\n",
        "\n",
        "\n",
        "def delnan(x_train, x_test):\n",
        "  '''\n",
        "  Replace all values causing errors by NaN, and replace those and pre-\n",
        "  existing NaNs by the column's median\n",
        "  '''\n",
        "  data_columns_train = list(set(x_train))\n",
        "  data_columns_test = list(set(x_test))\n",
        "  ## Replace inf en -inf by NaN\n",
        "  x_inf_train = x_train.replace([np.inf, -np.inf], np.nan)\n",
        "  x_inf_test = x_test.replace([np.inf, -np.inf], np.nan)\n",
        "  ## Replace strings by NaN\n",
        "  x_str_train = (x_inf_train.drop(data_columns_train, axis=1)\n",
        "             .join(x_inf_train[data_columns_train].apply(pd.to_numeric, errors='coerce')))\n",
        "  x_str_test = (x_inf_test.drop(data_columns_test, axis=1)\n",
        "             .join(x_inf_test[data_columns_test].apply(pd.to_numeric, errors='coerce')))\n",
        "  ## Delete all columns containing over 50% NaN\n",
        "  x_del_nan_train = x_str_train.dropna(axis='columns', thresh= round(0.5 * len(x_str_train)))\n",
        "  x_del_nan_test = x_str_test.dropna(axis='columns', thresh= round(0.5 * len(x_str_test)))\n",
        "  ## Replace all NaNs with the trainings set column's median\n",
        "  x_finished_train = x_del_nan_train.fillna(x_del_nan_train.median())\n",
        "  x_finished_test = x_del_nan_test.fillna(x_del_nan_train.median())\n",
        "  \n",
        "  return x_finished_train, x_finished_test\n",
        "\n",
        "\n",
        "def standardscaler(x_train, x_test):\n",
        "  '''\n",
        "  Scale all values using standard scaling\n",
        "  '''\n",
        "  ## Design scaler\n",
        "  scaler = preprocessing.StandardScaler()\n",
        "  # scaler = preprocessing.RobustScaler(quantile_range=[5, 95])\n",
        "  scaler.fit(x_train)\n",
        "\n",
        "  ## Apply scaler to both sets and return scaled sets\n",
        "  x_scaled_train = scaler.transform(x_train)\n",
        "  x_scaled_test = scaler.transform(x_test)\n",
        "\n",
        "  x_scaled_df_train = pd.DataFrame(x_scaled_train, columns=x_train.columns)\n",
        "  x_scaled_df_test = pd.DataFrame(x_scaled_test, columns=x_test.columns)\n",
        "\n",
        "  return x_scaled_df_train, x_scaled_df_test\n",
        "\n",
        "def pair_plot(x,y,features):\n",
        "  '''\n",
        "  Plot the selected features using a pairplot\n",
        "  '''\n",
        "  x_pairplot = pd.DataFrame(x)\n",
        "  x_pairplot.columns = features\n",
        "  y_pairplot = pd.DataFrame(y, columns=['label'])\n",
        "  y_pairplot = y_pairplot.reset_index(drop=True)\n",
        "  total = pd.concat([x_pairplot, y_pairplot], axis=1)\n",
        "  # fig = seaborn.pairplot(total, hue='label')\n",
        "\n",
        "\n",
        "def print_result(result, feature, multiply):\n",
        "  '''\n",
        "  Print the result dataframe\n",
        "  Print the prevalence of each classifier in the result dataframe\n",
        "  Print the mean accuracy of the overall machine learning algorithm\n",
        "  --------------\n",
        "  print the features used in the cross validation\n",
        "  '''\n",
        "  print('Results'+'\\n'+'-'*80)\n",
        "  print(result)\n",
        "  print('='*80+'\\n'+'Results over all iterations:')\n",
        "  print('Mean AUC:',result['Area Under the Curve'].mean(), \n",
        "        '(min:', result['Area Under the Curve'].min(), 'max:', \n",
        "        result['Area Under the Curve'].max(), ')')\n",
        "  print('Sensitivity:',result['Sensitivity'].mean(),\n",
        "        '(min:', result['Sensitivity'].min(), 'max:', \n",
        "        result['Sensitivity'].max(), ')')\n",
        "  print('Specificity:',result['Specificity'].mean(), \n",
        "        '(min:', result['Specificity'].min(), 'max:', \n",
        "        result['Specificity'].max(), ')')\n",
        "  print('='*80)\n",
        "  if multiply[0].upper() == 'UNIVARIATE':\n",
        "    print('Prevalence of selected features:'+'\\n'+'-'*80+'\\n')\n",
        "    print(feature['Feature'].value_counts(normalize=True)*100*multiply[1])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZwdQJr3Qlxz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Model optimization functions\n",
        "'''\n",
        "\n",
        "\n",
        "def classif_hyperpar(x_train, y_train, feat_name):\n",
        "  optim_classif = pd.DataFrame(columns=['clf_name', feat_name])\n",
        "  train_auc = []\n",
        "  classif_name = []\n",
        "\n",
        "  ## Random Forest classifier optimization\n",
        "  number_of_trees = [10, 30, 100, 200]\n",
        "  for tree in number_of_trees:\n",
        "    clf = RandomForestClassifier(n_estimators=tree)\n",
        "    scores = cross_val_score(clf, x_train, y_train, cv=5, scoring='roc_auc')\n",
        "    mean_scores = scores.mean()\n",
        "    train_auc.append(mean_scores)\n",
        "    classif_name.append(f'RF {tree}')\n",
        "    \n",
        "  ## SVM classifier optimization\n",
        "  slacks = [0.3, 0.1, 0.05]\n",
        "  kernels = ['linear', 'rbf', 'poly']\n",
        "  for kern in kernels:\n",
        "    for slack in slacks:\n",
        "      clf = svm.SVC(C= slack, kernel= kern)\n",
        "      scores = cross_val_score(clf, x_train, y_train, cv=5, scoring='roc_auc')\n",
        "      mean_scores = scores.mean()\n",
        "      train_auc.append(mean_scores)\n",
        "      classif_name.append(f'SVM {kern} {slack}')\n",
        "\n",
        "  ## k-nearest neighbors classifier optimization\n",
        "  number_of_neighbors = [3, 7, 11, 15]\n",
        "  for neighbor in number_of_neighbors:\n",
        "    clf = KNeighborsClassifier(n_neighbors= neighbor)\n",
        "    scores = cross_val_score(clf, x_train, y_train, cv=5, scoring='roc_auc')\n",
        "    mean_scores = scores.mean()\n",
        "    train_auc.append(mean_scores)\n",
        "    classif_name.append(f'KNN {neighbor}')\n",
        "\n",
        "  optim_classif['clf_name'] = classif_name\n",
        "  optim_classif[feat_name] = train_auc\n",
        "\n",
        "  return optim_classif\n",
        "\n",
        "  \n",
        "def features_hyperpar(x_train, y_train):\n",
        "  '''\n",
        "  Select discriminating features using PCA & univariate feature selection \n",
        "  '''\n",
        "  data_optim = pd.DataFrame()\n",
        "\n",
        "  ## PCA feature selection optimization\n",
        "  pca = PCA(n_components=None)\n",
        "  pca.fit(x_train)\n",
        "\n",
        "  variancelist = np.cumsum(pca.explained_variance_ratio_)\n",
        "\n",
        "  ## Thresholds used in gridsearch\n",
        "  thresholds = [0.5, 0.75, 0.9, 0.95]\n",
        "  for thres in thresholds:\n",
        "    comp = np.searchsorted(variancelist, thres)\n",
        "\n",
        "    pca_spec = PCA(n_components=comp)\n",
        "    pca_spec.fit(x_train)\n",
        "  \n",
        "    ## Use PCA for all different classifiers to be optimized\n",
        "    x_pca_train = pd.DataFrame(pca_spec.transform(x_train))\n",
        "    PCA_name = f'PCA {thres}'\n",
        "    PCA_classif = classif_hyperpar(x_pca_train, y_train, PCA_name)\n",
        "    if data_optim.empty:\n",
        "      data_optim = PCA_classif\n",
        "    elif data_optim.empty is False:\n",
        "      data_optim = data_optim.merge(PCA_classif, how='outer', on='clf_name')\n",
        "\n",
        "  ## Univariate feature selection (kbest) optimization\n",
        "  ## thresholds used in gridsearch \n",
        "  number_of_features = [5, 10, 25, 50, 100]\n",
        "  for number in number_of_features:  \n",
        "    ## Apply kbest, with given threshold, for all different classifiers to be optimized\n",
        "    Kbest = SelectKBest(f_classif, k=number).fit(x_train, y_train)\n",
        "    x_Kbest_train = pd.DataFrame(Kbest.transform(x_train))\n",
        "    \n",
        "    Kbest_name = f'Univariate {number}'\n",
        "    Kbest_classif = classif_hyperpar(x_Kbest_train, y_train, Kbest_name)\n",
        "    if data_optim.empty:\n",
        "      data_optim = Kbest_classif\n",
        "    elif data_optim.empty is False:\n",
        "      data_optim = data_optim.merge(Kbest_classif, how='outer', on='clf_name')\n",
        "\n",
        "  return data_optim\n",
        "\n",
        "\n",
        "def gridsearch (data):\n",
        "  '''\n",
        "  perform a manual gridsearch to perform model optimization\n",
        "  '''\n",
        "  data_train, data_test, labels_train, labels_test = split(data)\n",
        "  x_train, x_test = delnan(data_train, data_test)\n",
        "  x_scaled_train, x_scaled_test = standardscaler(x_train, x_test)\n",
        "  data_optim = features_hyperpar(x_scaled_train, labels_train)\n",
        "  \n",
        "  ## print and save results\n",
        "  pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "  print('Results')\n",
        "  print(data_optim)\n",
        "  \n",
        "  avg = data_optim.mean()\n",
        "  print('-'*80)\n",
        "  print(f'Mean performance for each feature selection method:\\n{avg}')\n",
        "  \n",
        "  maximum = avg.idxmax()\n",
        "  print('-'*80)\n",
        "  print(f'Feature selection method with best mean performance:\\n{maximum}')\n",
        "  \n",
        "  clf = data_optim[str(maximum)].idxmax()\n",
        "  value = data_optim[str(maximum)].max()\n",
        "  print('-'*80)\n",
        "  print(f'Classifier with best performance using {maximum}:')\n",
        "  print(f\"{data_optim['clf_name'][clf]}: {value}\")\n",
        "  print('-'*80)\n",
        "  print(f\"Below, fill in {maximum} and {data_optim['clf_name'][clf]}\")\n",
        "\n",
        "  ## Save results\n",
        "  path = Path('optimization_TM10007.csv')\n",
        "  data_optim.to_csv(path)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcwBJnEwRqfy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Classifier performance evaluation functions\n",
        "'''\n",
        "def select_features(x_train, x_test, y_train, parameters):\n",
        "  '''\n",
        "  Select discriminating features using PCA & univariate feature selection \n",
        "  '''\n",
        "  ## PCA feature selection\n",
        "  if parameters[0].upper() == 'PCA':\n",
        "    pca = PCA(n_components=None)\n",
        "    pca.fit(x_train)\n",
        "\n",
        "    variancelist = np.cumsum(pca.explained_variance_ratio_)\n",
        "\n",
        "    ## Determine the amount of features within the given threshold\n",
        "    comp = np.searchsorted(variancelist, parameters[1])\n",
        "\n",
        "    pca_spec = PCA(n_components=comp)\n",
        "    pca_spec.fit(x_train)\n",
        "    \n",
        "    ## Apply PCA to the different sets\n",
        "    x_feat_selected_train = pd.DataFrame(pca_spec.transform(x_train))\n",
        "    x_feat_selected_test = pd.DataFrame(pca_spec.transform(x_test))\n",
        "\n",
        "    \n",
        "    return x_feat_selected_train, x_feat_selected_test, _\n",
        "  \n",
        "  ## Select the best features and apply them to the different sets\n",
        "  elif parameters[0].upper() == 'UNIVARIATE':\n",
        "    Kbest = SelectKBest(f_classif, k=parameters[1]).fit(x_train, y_train)\n",
        "    x_feat_selected_train = pd.DataFrame(Kbest.transform(x_train))\n",
        "    x_feat_selected_test = pd.DataFrame(Kbest.transform(x_test))\n",
        "\n",
        "    ## determine the used features\n",
        "    feature_names = list(x_train.columns.values)\n",
        "    mask = Kbest.get_support() #list of booleans\n",
        "    used_features = [] # The list of the best features\n",
        "\n",
        "    for bool, feature in zip(mask, feature_names):\n",
        "      if bool:\n",
        "          used_features.append(feature)\n",
        "\n",
        "    return x_feat_selected_train, x_feat_selected_test, used_features\n",
        "\n",
        "\n",
        "def clf_test(x_train, x_test, y_train,  y_test, parameters):\n",
        "  '''\n",
        "   Evaluate classifier performance\n",
        "  '''\n",
        "  ## RF classifier\n",
        "  if parameters[2].upper() == 'RF':\n",
        "    clf = RandomForestClassifier(n_estimators=parameters[3])\n",
        "    clf.fit(x_train, y_train)\n",
        "    y_pred = clf.predict_proba(x_test)[:,1]\n",
        "    auc = roc_auc_score(y_test, y_pred)\n",
        "    y_pred_sens = clf.predict(x_test)\n",
        "    matrix = confusion_matrix(y_test, y_pred_sens)\n",
        "    sens = matrix[0,0] / (matrix[0,0]+matrix[0,1])\n",
        "    spec = matrix[1,1] / (matrix[1,0]+matrix[1,1])\n",
        "\n",
        "    perf = [auc, sens, spec]\n",
        "    return perf\n",
        "\n",
        "  ## SVM classifier\n",
        "  elif parameters[2].upper() == 'SVM':\n",
        "    clf = svm.SVC(C= parameters[4], kernel= parameters[5], probability=True)\n",
        "    clf.fit(x_train, y_train)\n",
        "    y_pred = clf.predict_proba(x_test)[:,1]\n",
        "    auc = roc_auc_score(y_test, y_pred)\n",
        "    y_pred_sens = clf.predict(x_test)\n",
        "    matrix = confusion_matrix(y_test, y_pred_sens)\n",
        "    sens = matrix[0,0] / (matrix[0,0]+matrix[0,1])\n",
        "    spec = matrix[1,1] / (matrix[1,0]+matrix[1,1])\n",
        "\n",
        "    perf = [auc, sens, spec]\n",
        "    return perf\n",
        "\n",
        "  ## KNN classifier\n",
        "  elif parameters[2].upper() == 'KNN':\n",
        "    clf = KNeighborsClassifier(n_neighbors=parameters[6])\n",
        "    clf.fit(x_train, y_train)\n",
        "    y_pred=clf.predict_proba(x_test)[:,1]\n",
        "    auc = roc_auc_score(y_test, y_pred)\n",
        "    y_pred_sens = clf.predict(x_test)\n",
        "    matrix = confusion_matrix(y_test, y_pred_sens)\n",
        "    sens = matrix[0,0] / (matrix[0,0]+matrix[0,1])\n",
        "    spec = matrix[1,1] / (matrix[1,0]+matrix[1,1])\n",
        "\n",
        "    perf = [auc, sens, spec]\n",
        "    return perf\n",
        "\n",
        "\n",
        "def test_performance(x_train, x_test, y_train, y_test, parameters):\n",
        "  '''\n",
        "  perform all steps required to test optimized model performance\n",
        "  '''\n",
        "\n",
        "  ## use given scaling method, clf and hyperparameters to test performance\n",
        "  x_train, x_test = delnan(x_train, x_test)\n",
        "  x_train, x_test = standardscaler(x_train, x_test)\n",
        "  x_train, x_test, features = select_features(x_train, x_test, y_train, \n",
        "                                             parameters)\n",
        "  # pair_plot(x_train, y_train, features)\n",
        "  perf = clf_test(x_train, x_test, y_train, y_test, parameters)\n",
        "\n",
        "  return clf, perf, features\n",
        "\n",
        "\n",
        "def start_test(data, iterations, feature_method, feature_threshold, clf, \n",
        "                trees, slack, kernel, neighbors):\n",
        "\n",
        "  iteration = 0\n",
        "  parameters = [feature_method, feature_threshold, clf, \n",
        "                trees, slack, kernel, neighbors]\n",
        "  outcome = pd.DataFrame(columns=['Area Under the Curve', 'Sensitivity', \n",
        "                                  'Specificity'])\n",
        "  used_features = pd.DataFrame(columns=['Feature'])\n",
        "  \n",
        "  ## Run the script the desired amount of iterations\n",
        "  while iteration < iterations:\n",
        "    x_train, x_test, y_train, y_test = split(data)\n",
        "    clf, perf, features = test_performance(x_train, x_test, y_train, \n",
        "                                           y_test, parameters)\n",
        "    \n",
        "    ## add result of iteration to dataframe\n",
        "    add_result = {'Area Under the Curve': perf[0],\n",
        "                  'Sensitivity': perf[1], 'Specificity': perf[2]}\n",
        "    outcome = outcome.append(add_result, ignore_index=True)\n",
        "    \n",
        "    ## add features used in iteration to dataframe\n",
        "    for feature in features:\n",
        "      add_feature = {'Feature': feature}\n",
        "      used_features = used_features.append(add_feature, ignore_index=True)\n",
        "      \n",
        "    iteration += 1\n",
        "\n",
        "  ## Print results\n",
        "  print_result(outcome, used_features, parameters)\n",
        "\n",
        "  ## Save results\n",
        "  path = Path('test_results_TM10007.csv')\n",
        "  outcome.to_csv(path)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxWdQ7Y0mH1p",
        "colab_type": "code",
        "outputId": "d7f31c7d-328c-4064-b72b-e2aa7f5efb7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 991
        }
      },
      "source": [
        "'''\n",
        "run all steps required for hyperparameter optimization\n",
        "and print the dataframe containing grid search results.\n",
        "\n",
        "Use the results in the following steps\n",
        "'''\n",
        "gridsearch(data)"
      ],
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results\n",
            "           clf_name   PCA 0.5  PCA 0.75   PCA 0.9  PCA 0.95  Univariate 5  \\\n",
            "0             RF 10  0.858813  0.856806  0.664230  0.728321      0.917816   \n",
            "1             RF 30  0.879015  0.859621  0.802437  0.802273      0.924848   \n",
            "2            RF 100  0.866869  0.873283  0.848081  0.786225      0.921098   \n",
            "3            RF 200  0.873056  0.882904  0.845379  0.837235      0.921755   \n",
            "4    SVM linear 0.3  0.863485  0.897626  0.806591  0.874318      0.916212   \n",
            "5    SVM linear 0.1  0.863485  0.908182  0.816414  0.874318      0.909318   \n",
            "6   SVM linear 0.05  0.863485  0.905985  0.838434  0.877955      0.910631   \n",
            "7       SVM rbf 0.3  0.878030  0.890227  0.894621  0.899369      0.930379   \n",
            "8       SVM rbf 0.1  0.866616  0.889217  0.894924  0.896035      0.921111   \n",
            "9      SVM rbf 0.05  0.866616  0.889217  0.894924  0.896035      0.928131   \n",
            "10     SVM poly 0.3  0.808030  0.844444  0.845758  0.849116      0.911894   \n",
            "11     SVM poly 0.1  0.818485  0.852424  0.845404  0.846616      0.906237   \n",
            "12    SVM poly 0.05  0.833030  0.854949  0.850152  0.846616      0.907449   \n",
            "13            KNN 3  0.848106  0.882904  0.879798  0.871970      0.874571   \n",
            "14            KNN 7  0.890189  0.864659  0.870997  0.860985      0.883939   \n",
            "15           KNN 11  0.875972  0.879470  0.888826  0.896705      0.877500   \n",
            "16           KNN 15  0.882323  0.892412  0.889520  0.889066      0.892134   \n",
            "\n",
            "    Univariate 10  Univariate 25  Univariate 50  Univariate 100  \n",
            "0        0.918384       0.918990       0.936149        0.925265  \n",
            "1        0.959482       0.938750       0.936490        0.921894  \n",
            "2        0.952992       0.945480       0.936578        0.946490  \n",
            "3        0.955278       0.950821       0.940455        0.940366  \n",
            "4        0.922803       0.904545       0.880657        0.900076  \n",
            "5        0.920556       0.908106       0.893939        0.898737  \n",
            "6        0.917172       0.903561       0.904571        0.886086  \n",
            "7        0.941641       0.944470       0.946566        0.943283  \n",
            "8        0.913864       0.941010       0.947727        0.945732  \n",
            "9        0.909318       0.942121       0.947727        0.945732  \n",
            "10       0.899672       0.874495       0.911818        0.908788  \n",
            "11       0.891490       0.893308       0.911010        0.902273  \n",
            "12       0.892803       0.900404       0.913434        0.902298  \n",
            "13       0.898864       0.915593       0.886667        0.886035  \n",
            "14       0.897879       0.896136       0.896389        0.910985  \n",
            "15       0.907992       0.889293       0.904407        0.900429  \n",
            "16       0.910101       0.892551       0.922790        0.912159  \n",
            "--------------------------------------------------------------------------------\n",
            "Mean performance for each feature selection method:\n",
            "PCA 0.5           0.860918\n",
            "PCA 0.75          0.877902\n",
            "PCA 0.9           0.845676\n",
            "PCA 0.95          0.854892\n",
            "Univariate 5      0.909119\n",
            "Univariate 10     0.918252\n",
            "Univariate 25     0.915273\n",
            "Univariate 50     0.918669\n",
            "Univariate 100    0.916272\n",
            "dtype: float64\n",
            "--------------------------------------------------------------------------------\n",
            "Feature selection method with best mean performance:\n",
            "Univariate 50\n",
            "--------------------------------------------------------------------------------\n",
            "Classifier with best performance using Univariate 50:\n",
            "SVM rbf 0.1: 0.9477272727272726\n",
            "--------------------------------------------------------------------------------\n",
            "Below, fill in Univariate 50 and SVM rbf 0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stvuPA2GbmUd",
        "colab_type": "code",
        "outputId": "02fd3a58-2412-42d5-9e33-63242f3a476e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "'''\n",
        "test the optimized classifier \"iterations\" times\n",
        "\n",
        "Fill in the feature selection method, classifier and optimized hyperparamaters based on \n",
        "the gridsearch above\n",
        "\n",
        "feature_method = type of feature method ('pca', 'univariate')\n",
        "feature_threshold = hyperparameter for specific feature selection method\n",
        "                    when using pca: (0.5, 0.75, 0.9, 0.95)\n",
        "                    when using univariate: (5, 10, 25, 50, 100)\n",
        "clf = type of classifier ('RF', 'SVM', 'KNN')\n",
        "trees = number of trees in RF (10, 30, 100, 200)\n",
        "slack = slack value in SVM (0.05, 0.1, '0.3)\n",
        "kernel = kernel in SVM ('rbf', 'poly', 'linear')\n",
        "neighbors = number of neighbors in KNN (3, 7, 11, 15)\n",
        "\n",
        "if no value is needed, fill in 0\n",
        "'''\n",
        "feature_method = 'univariate'\n",
        "feature_threshold = 50\n",
        "clf = 'svm' \n",
        "trees = 0\n",
        "slack = 0.1\n",
        "kernel = 'rbf'\n",
        "neighbors = 0\n",
        "\n",
        "iterations = 100\n",
        "\n",
        "start_test(data, iterations, feature_method, feature_threshold, clf, trees, slack, kernel, neighbors)"
      ],
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results\n",
            "--------------------------------------------------------------------------------\n",
            "    Area Under the Curve  Sensitivity  Specificity\n",
            "0               0.996429     1.000000     0.785714\n",
            "1               0.831579     0.789474     0.666667\n",
            "2               0.967033     1.000000     0.769231\n",
            "3               0.915556     0.880000     0.888889\n",
            "4               0.951111     1.000000     0.666667\n",
            "5               0.963370     0.952381     0.846154\n",
            "6               0.919298     0.894737     0.800000\n",
            "7               0.939394     1.000000     0.750000\n",
            "8               0.975000     0.950000     0.928571\n",
            "9               0.910714     0.950000     0.571429\n",
            "10              0.924901     0.956522     0.909091\n",
            "11              0.971429     1.000000     0.571429\n",
            "12              0.889328     0.956522     0.545455\n",
            "13              0.967033     1.000000     0.538462\n",
            "14              0.909091     0.954545     0.583333\n",
            "15              0.928030     0.909091     0.750000\n",
            "16              0.900000     0.900000     0.785714\n",
            "17              0.923611     1.000000     0.277778\n",
            "18              0.882143     0.850000     0.785714\n",
            "19              0.982456     1.000000     0.533333\n",
            "20              0.988636     1.000000     0.833333\n",
            "21              0.942857     1.000000     0.785714\n",
            "22              0.972222     1.000000     0.750000\n",
            "23              0.916667     0.954545     0.833333\n",
            "24              0.901515     0.909091     0.583333\n",
            "25              0.920949     1.000000     0.636364\n",
            "26              0.948718     0.952381     0.769231\n",
            "27              0.937729     0.952381     0.692308\n",
            "28              0.861592     0.882353     0.588235\n",
            "29              0.992982     1.000000     0.800000\n",
            "30              0.932143     1.000000     0.714286\n",
            "31              0.954167     0.958333     0.800000\n",
            "32              0.977273     1.000000     0.500000\n",
            "33              0.951557     1.000000     0.470588\n",
            "34              0.991667     1.000000     0.700000\n",
            "35              0.952381     1.000000     0.692308\n",
            "36              0.950877     1.000000     0.533333\n",
            "37              0.975000     1.000000     0.714286\n",
            "38              0.965909     0.954545     0.666667\n",
            "39              0.989011     1.000000     0.846154\n",
            "40              0.903571     0.928571     0.500000\n",
            "41              0.992424     1.000000     0.916667\n",
            "42              0.981685     1.000000     0.769231\n",
            "43              0.974359     1.000000     0.692308\n",
            "44              0.940972     0.937500     0.777778\n",
            "45              1.000000     0.923077     1.000000\n",
            "46              0.920139     0.888889     0.812500\n",
            "47              0.968858     1.000000     0.529412\n",
            "48              0.962121     1.000000     0.666667\n",
            "49              0.968421     1.000000     0.533333\n",
            "50              0.916667     1.000000     0.555556\n",
            "51              0.934066     1.000000     0.615385\n",
            "52              0.924242     0.954545     0.833333\n",
            "53              0.859848     1.000000     0.666667\n",
            "54              0.962500     0.958333     0.900000\n",
            "55              0.936842     0.894737     0.866667\n",
            "56              0.958333     0.875000     0.900000\n",
            "57              0.951389     0.944444     0.562500\n",
            "58              0.931818     0.909091     0.583333\n",
            "59              0.989583     1.000000     0.812500\n",
            "60              0.877193     0.947368     0.600000\n",
            "61              0.954861     1.000000     0.562500\n",
            "62              0.940351     0.947368     0.533333\n",
            "63              0.960714     1.000000     0.714286\n",
            "64              0.930556     0.944444     0.625000\n",
            "65              0.904762     0.904762     0.846154\n",
            "66              0.910714     0.950000     0.571429\n",
            "67              0.951111     0.880000     1.000000\n",
            "68              0.962500     0.958333     0.900000\n",
            "69              0.953571     1.000000     0.357143\n",
            "70              0.954861     1.000000     0.611111\n",
            "71              0.961404     1.000000     0.400000\n",
            "72              0.943860     1.000000     0.600000\n",
            "73              0.906574     0.941176     0.823529\n",
            "74              0.937778     0.920000     1.000000\n",
            "75              0.977273     1.000000     0.583333\n",
            "76              0.943182     0.863636     0.833333\n",
            "77              0.888889     0.944444     0.687500\n",
            "78              0.926316     0.947368     0.466667\n",
            "79              0.975694     1.000000     0.444444\n",
            "80              0.932806     0.956522     0.818182\n",
            "81              0.897727     0.909091     0.666667\n",
            "82              1.000000     1.000000     0.421053\n",
            "83              0.933333     1.000000     0.600000\n",
            "84              0.948617     0.913043     0.818182\n",
            "85              0.996429     1.000000     0.357143\n",
            "86              0.956044     1.000000     0.692308\n",
            "87              0.951389     1.000000     0.437500\n",
            "88              0.961938     1.000000     0.764706\n",
            "89              0.958333     0.909091     0.916667\n",
            "90              0.890152     0.909091     0.833333\n",
            "91              0.920139     0.944444     0.812500\n",
            "92              0.912088     0.952381     0.769231\n",
            "93              0.934028     0.944444     0.687500\n",
            "94              0.988142     0.956522     0.909091\n",
            "95              0.993080     1.000000     0.411765\n",
            "96              0.975000     0.833333     0.900000\n",
            "97              0.952569     0.913043     0.818182\n",
            "98              0.895833     0.937500     0.666667\n",
            "99              0.950758     0.954545     0.833333\n",
            "================================================================================\n",
            "Results over all iterations:\n",
            "Mean AUC: 0.9435976531852529 (min: 0.8315789473684211 max: 1.0 )\n",
            "Sensitivity: 0.9590304355767469 (min: 0.7894736842105263 max: 1.0 )\n",
            "Specificity: 0.6965070761711629 (min: 0.2777777777777778 max: 1.0 )\n",
            "================================================================================\n",
            "Prevalence of selected features:\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "VOLUME_ET                                100.0\n",
            "VOLUME_ET_OVER_BRAIN                     100.0\n",
            "VOLUME_NET_over_TC                       100.0\n",
            "TEXTURE_GLRLM_ET_T2_LRHGE                100.0\n",
            "TEXTURE_GLRLM_ET_T1Gd_GLV                100.0\n",
            "TEXTURE_GLRLM_ET_T1Gd_SRE                100.0\n",
            "VOLUME_NET                               100.0\n",
            "VOLUME_NET_OVER_BRAIN                    100.0\n",
            "TEXTURE_GLRLM_ET_T1Gd_RLN                100.0\n",
            "TEXTURE_GLCM_ET_T2_SumAverage            100.0\n",
            "TEXTURE_GLRLM_ET_T1Gd_LRE                100.0\n",
            "TEXTURE_GLRLM_ET_FLAIR_GLV               100.0\n",
            "VOLUME_NET_OVER_WT                       100.0\n",
            "VOLUME_ET_OVER_WT                        100.0\n",
            "TEXTURE_GLCM_ET_T2_AutoCorrelation       100.0\n",
            "TEXTURE_GLSZM_ET_T1Gd_ZP                 100.0\n",
            "TEXTURE_GLRLM_ET_T1Gd_RP                 100.0\n",
            "VOLUME_ET_over_TC                        100.0\n",
            "TEXTURE_NGTDM_NET_T2_Busyness            100.0\n",
            "SOLIDITY_NET                             100.0\n",
            "TEXTURE_GLRLM_ET_FLAIR_LRHGE              99.0\n",
            "TEXTURE_GLCM_ET_T2_Variance               93.0\n",
            "TEXTURE_GLSZM_ET_FLAIR_LZLGE              90.0\n",
            "TEXTURE_GLRLM_ET_T1_GLV                   86.0\n",
            "TEXTURE_GLSZM_NET_T1Gd_SZE                85.0\n",
            "TEXTURE_GLRLM_ET_FLAIR_RLV                85.0\n",
            "TEXTURE_GLRLM_ET_T2_HGRE                  82.0\n",
            "TEXTURE_GLCM_ED_T1Gd_SumAverage           80.0\n",
            "TEXTURE_GLRLM_ET_T1_LRE                   79.0\n",
            "TEXTURE_GLSZM_ET_T1_ZP                    77.0\n",
            "TEXTURE_GLRLM_ET_T1_RP                    75.0\n",
            "TEXTURE_NGTDM_ET_T2_Busyness              72.0\n",
            "TEXTURE_GLRLM_ED_T1Gd_LRLGE               72.0\n",
            "INTENSITY_STD_ET_T2                       71.0\n",
            "TEXTURE_GLSZM_ET_T2_GLN                   71.0\n",
            "TEXTURE_GLSZM_ET_FLAIR_LZHGE              70.0\n",
            "TEXTURE_GLRLM_ET_T2_SRHGE                 60.0\n",
            "TEXTURE_GLSZM_ET_T1_LGZE                  59.0\n",
            "TEXTURE_GLRLM_ET_T1_RLN                   58.0\n",
            "TEXTURE_GLCM_ED_T1_SumAverage             57.0\n",
            "TEXTURE_GLSZM_ET_T2_LGZE                  56.0\n",
            "TEXTURE_GLSZM_NET_T1Gd_ZSN                55.0\n",
            "TEXTURE_GLCM_ET_FLAIR_AutoCorrelation     54.0\n",
            "VOLUME_NET_OVER_ED                        54.0\n",
            "TEXTURE_GLRLM_ET_T1_SRE                   53.0\n",
            "TEXTURE_GLRLM_ET_T2_GLN                   52.0\n",
            "TEXTURE_GLRLM_ET_FLAIR_LRLGE              51.0\n",
            "TEXTURE_NGTDM_ET_FLAIR_Busyness           49.0\n",
            "TEXTURE_GLCM_ED_T1Gd_AutoCorrelation      48.0\n",
            "TEXTURE_GLRLM_ET_FLAIR_LRE                43.0\n",
            "TEXTURE_GLSZM_ET_FLAIR_LGZE               41.0\n",
            "TEXTURE_GLRLM_ED_T1Gd_SRLGE               38.0\n",
            "TEXTURE_GLRLM_NET_T1_LRHGE                34.0\n",
            "TEXTURE_GLCM_ET_FLAIR_Variance            32.0\n",
            "TEXTURE_GLCM_ED_T1Gd_Contrast             30.0\n",
            "TEXTURE_NGTDM_ED_T1Gd_Contrast            27.0\n",
            "TEXTURE_NGTDM_NET_FLAIR_Busyness          26.0\n",
            "TEXTURE_GLRLM_ET_T2_RLV                   26.0\n",
            "TEXTURE_GLSZM_ET_T1Gd_SZE                 26.0\n",
            "TEXTURE_GLCM_NET_T1Gd_AutoCorrelation     25.0\n",
            "TEXTURE_GLCM_ED_T1Gd_Dissimilarity        24.0\n",
            "TEXTURE_GLRLM_ET_T1_LRLGE                 23.0\n",
            "TEXTURE_GLCM_ET_FLAIR_SumAverage          22.0\n",
            "TEXTURE_NGTDM_ED_T1Gd_Complexity          22.0\n",
            "TEXTURE_GLSZM_ED_T1Gd_ZP                  21.0\n",
            "TEXTURE_NGTDM_ET_FLAIR_Strength           20.0\n",
            "TEXTURE_GLCM_ET_T1Gd_Homogeneity          20.0\n",
            "HISTO_NET_FLAIR_Bin8                      20.0\n",
            "TEXTURE_GLSZM_ET_T2_HGZE                  19.0\n",
            "VOLUME_ET_OVER_ED                         18.0\n",
            "TEXTURE_NGTDM_ET_T1Gd_Busyness            18.0\n",
            "TEXTURE_GLRLM_ET_T1_SRLGE                 18.0\n",
            "TEXTURE_GLSZM_ET_T1Gd_ZSN                 17.0\n",
            "TEXTURE_GLSZM_NET_T1Gd_ZP                 17.0\n",
            "TEXTURE_NGTDM_ET_T1_Busyness              17.0\n",
            "TEXTURE_GLCM_ED_T1Gd_Entropy              16.0\n",
            "TEXTURE_GLSZM_ED_T1Gd_SZLGE               16.0\n",
            "TEXTURE_GLCM_NET_T1_SumAverage            16.0\n",
            "TEXTURE_GLCM_ET_T2_Entropy                15.0\n",
            "TEXTURE_GLCM_ET_T1Gd_Correlation          14.0\n",
            "TEXTURE_GLRLM_ET_FLAIR_RP                 14.0\n",
            "TEXTURE_GLSZM_NET_T2_LZLGE                14.0\n",
            "TEXTURE_GLRLM_ET_T1_RLV                   13.0\n",
            "TEXTURE_GLRLM_ET_T2_SRLGE                 13.0\n",
            "TEXTURE_GLCM_ED_T1_AutoCorrelation        12.0\n",
            "TEXTURE_GLRLM_ED_T1_GLV                   12.0\n",
            "TEXTURE_GLCM_NET_T1_AutoCorrelation       12.0\n",
            "TEXTURE_GLCM_ET_T1_Correlation            12.0\n",
            "TEXTURE_GLRLM_ET_FLAIR_SRLGE              11.0\n",
            "TEXTURE_GLRLM_ET_T2_GLV                   11.0\n",
            "TEXTURE_GLSZM_ET_T1Gd_LZHGE               10.0\n",
            "TEXTURE_GLCM_NET_T1Gd_SumAverage          10.0\n",
            "TEXTURE_GLSZM_ET_T1_SZLGE                  9.0\n",
            "TEXTURE_GLRLM_NET_T2_LRLGE                 9.0\n",
            "TEXTURE_NGTDM_NET_T1_Busyness              9.0\n",
            "TEXTURE_GLRLM_ED_T1Gd_GLV                  8.0\n",
            "TEXTURE_GLSZM_ET_T1_LZLGE                  7.0\n",
            "TEXTURE_NGTDM_ET_T1Gd_Strength             7.0\n",
            "TEXTURE_NGTDM_ET_FLAIR_Coarseness          6.0\n",
            "TEXTURE_GLSZM_ET_T2_ZSV                    6.0\n",
            "TEXTURE_GLSZM_ET_T2_LZLGE                  5.0\n",
            "TEXTURE_GLRLM_ED_T1Gd_LGRE                 5.0\n",
            "TEXTURE_NGTDM_NET_T1Gd_Busyness            5.0\n",
            "INTENSITY_STD_NET_T2                       5.0\n",
            "TEXTURE_GLRLM_ET_T2_LGRE                   4.0\n",
            "TEXTURE_GLSZM_ET_T1_SZE                    4.0\n",
            "TEXTURE_GLRLM_ET_FLAIR_HGRE                4.0\n",
            "SOLIDITY_ET                                4.0\n",
            "TEXTURE_GLRLM_NET_T1Gd_RLN                 4.0\n",
            "TEXTURE_GLRLM_NET_T1Gd_SRE                 4.0\n",
            "TEXTURE_GLRLM_NET_T1Gd_RP                  4.0\n",
            "TEXTURE_GLRLM_NET_T1Gd_LRHGE               4.0\n",
            "HISTO_NET_T1Gd_Bin4                        4.0\n",
            "TEXTURE_GLSZM_ED_T1Gd_LGZE                 3.0\n",
            "TEXTURE_GLRLM_ET_FLAIR_LGRE                3.0\n",
            "TEXTURE_NGTDM_ET_T1_Strength               2.0\n",
            "TEXTURE_GLSZM_ET_T1_ZSN                    2.0\n",
            "INTENSITY_STD_ET_FLAIR                     1.0\n",
            "TEXTURE_GLOBAL_ET_T2_Kurtosis              1.0\n",
            "TEXTURE_NGTDM_ET_T2_Strength               1.0\n",
            "TEXTURE_GLRLM_ET_FLAIR_SRE                 1.0\n",
            "TEXTURE_GLRLM_ET_T1Gd_RLV                  1.0\n",
            "TEXTURE_GLCM_ED_T1Gd_Homogeneity           1.0\n",
            "TEXTURE_GLRLM_NET_T1Gd_LRE                 1.0\n",
            "TEXTURE_GLSZM_ET_T1Gd_GLV                  1.0\n",
            "TEXTURE_GLSZM_ET_T1_GLV                    1.0\n",
            "TEXTURE_GLRLM_ET_T1_LGRE                   1.0\n",
            "TEXTURE_GLSZM_ET_T2_SZHGE                  1.0\n",
            "TEXTURE_GLSZM_NET_T1Gd_LZLGE               1.0\n",
            "TEXTURE_GLSZM_ET_FLAIR_ZSV                 1.0\n",
            "INTENSITY_Mean_ET_T1Gd                     1.0\n",
            "TEXTURE_GLRLM_ED_T1_LRLGE                  1.0\n",
            "Name: Feature, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXgIfUJTWMx5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def crosval (x_train, y_train, x_test, y_test):\n",
        "  '''\n",
        "   Determine classifier performance using cross-validation\n",
        "  '''\n",
        "  ## Identify the classifiers used for cross validation\n",
        "  clfs = [svm.SVC(C=0.05 ,kernel='linear'), KNeighborsClassifier(n_neighbors=9), RandomForestClassifier(n_estimators=5, random_state=42, max_depth=4)]\n",
        "  clfs_name = ['Support Vector Machine','K Nearest Neighbors','Random Forest']\n",
        "\n",
        "  ## Start cross validation\n",
        "  # num = 0\n",
        "  train_acc = []\n",
        "\n",
        "  for clf in clfs:\n",
        "    ## Plot learning curve\n",
        "    # fig = plt.figure(figsize=(24,8*len(clfs)))\n",
        "    # ax=fig.add_subplot(4, 3, num + 1)\n",
        "    # num += 1\n",
        "    # plot_learning_curve(clf,str(type(clf)), x_train, y_train, axes=ax)\n",
        "\n",
        "    ## Calculate performance\n",
        "    scores = cross_val_score(clf, x_train, y_train, cv=5)\n",
        "    mean_scores = scores.mean()\n",
        "    train_acc.append(mean_scores)\n",
        "\n",
        "  ## Determine the best performing classifier\n",
        "  max_score = max(train_acc)\n",
        "  array = np.array(train_acc)\n",
        "  idx_max = np.argmax(array)\n",
        "  clf_max = clfs[idx_max]\n",
        "  clf_max_name = clfs_name[idx_max]\n",
        "\n",
        "  ## Apply chosen classifier on test data\n",
        "  clf_max.fit(x_train, y_train)\n",
        "  y_pred=clf_max.predict(x_test)\n",
        "  accuracy = accuracy_score(y_test, y_pred)\n",
        "  matrix = confusion_matrix(y_test, y_pred)\n",
        "  sens = matrix[0,0] / (matrix[0,0]+matrix[0,1])\n",
        "  spec = matrix[1,1] / (matrix[1,0]+matrix[1,1])\n",
        "\n",
        "  perf = [accuracy, sens, spec]\n",
        "\n",
        "  return clf_max_name, perf\n",
        "\n",
        "\n",
        "def start(data, iterations):\n",
        "  '''\n",
        "  Run the iterations. Print and save results.\n",
        "  '''\n",
        "  ## Run the script \"iterations\" time\n",
        "  iteration = 0\n",
        "  outcome = pd.DataFrame(columns=['Classifier', 'Accuracy','Sensitivity','Specificity'])\n",
        "  used_features = pd.DataFrame(columns=['Feature'])\n",
        "  while iteration < iterations:\n",
        "    x_train, x_test, y_train, y_test = split(data)\n",
        "    clf, perf, features = runscript(x_train, x_test, y_train, y_test)\n",
        "    \n",
        "    ## add results of iteration to dataframe\n",
        "    add_result = {'Classifier': str((clf)), 'Accuracy': perf[0],\n",
        "                  'Sensitivity': perf[1], 'Specificity': perf[2]}\n",
        "    outcome = outcome.append(add_result, ignore_index=True)\n",
        "    \n",
        "    ## add features used in iteration to dataframe\n",
        "    for feature in features:\n",
        "      add_feature = {'Feature': feature}\n",
        "      used_features = used_features.append(add_feature, ignore_index=True)\n",
        "      \n",
        "    iteration += 1\n",
        "  \n",
        "  ## Print results\n",
        "  print_result(outcome, used_features)\n",
        "  \n",
        "  ## Save results\n",
        "  path = Path('results_TM10007.csv')\n",
        "  outcome.to_csv(path)\n",
        "  \n",
        "  return outcome"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}