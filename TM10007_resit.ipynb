{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TM10007_resit.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JSSchouten/TM10007_Group_10/blob/master/TM10007_resit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AV8PjjK_vMDk",
        "colab_type": "code",
        "outputId": "a3d10d90-53ba-4549-da5b-2473e6bc3f3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Run this to use from colab environment\n",
        "!pip install -q --upgrade git+https://github.com/karinvangarderen/tm10007_project.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for brats (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFZRWeciabPz",
        "colab_type": "text"
      },
      "source": [
        "# Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCMSf2jf7Rut",
        "colab_type": "code",
        "outputId": "a88107c8-c8e3-4b34-9fef-62d69ea9032e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "'''\n",
        "all necessary imports\n",
        "'''\n",
        "# general functions\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "# load data\n",
        "from brats.load_data  import load_data\n",
        "\n",
        "# preprocessing and scaling\n",
        "from sklearn.model_selection    import train_test_split\n",
        "from sklearn                    import preprocessing\n",
        "from sklearn.decomposition      import PCA\n",
        "from sklearn.feature_selection  import SelectKBest, f_classif\n",
        "\n",
        "# classifiers\n",
        "from sklearn.model_selection        import cross_val_score, learning_curve\n",
        "from sklearn.neighbors              import KNeighborsClassifier\n",
        "from sklearn.ensemble               import RandomForestClassifier\n",
        "from sklearn                        import svm\n",
        "\n",
        "# calculate accuracy values\n",
        "from sklearn.metrics  import accuracy_score\n",
        "from sklearn.metrics  import confusion_matrix\n",
        "from sklearn.metrics  import roc_auc_score"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eq2zJcVgvCGE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Load the data from GitHub\n",
        "'''\n",
        "data = load_data()\n",
        "data_columns = list(set(data))\n",
        "# print(f'The number of samples: {len(data.index)}')\n",
        "# print(f'The number of columns: {len(data.columns)}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuaIFrUQQTxA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "General functions\n",
        "'''\n",
        "\n",
        "\n",
        "def split(data):\n",
        "  '''\n",
        "  Divide data in a training and test set 80% - 20%\n",
        "  '''\n",
        "  x = data.iloc[:,:-1]\n",
        "  y = data['label']\n",
        "  x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
        "\n",
        "  return x_train, x_test, y_train, y_test\n",
        "\n",
        "\n",
        "def delnan(x_train, x_test):\n",
        "  '''\n",
        "  Replace all values causing errors by NaN, and replace those and pre-\n",
        "  existing NaNs by the column's median\n",
        "  '''\n",
        "  data_columns_train = list(set(x_train))\n",
        "  data_columns_test = list(set(x_test))\n",
        "  ## Replace inf en -inf by NaN\n",
        "  x_inf_train = x_train.replace([np.inf, -np.inf], np.nan)\n",
        "  x_inf_test = x_test.replace([np.inf, -np.inf], np.nan)\n",
        "  ## Replace strings by NaN\n",
        "  x_str_train = (x_inf_train.drop(data_columns_train, axis=1)\n",
        "             .join(x_inf_train[data_columns_train].apply(pd.to_numeric, errors='coerce')))\n",
        "  x_str_test = (x_inf_test.drop(data_columns_test, axis=1)\n",
        "             .join(x_inf_test[data_columns_test].apply(pd.to_numeric, errors='coerce')))\n",
        "  ## Delete all columns containing over 50% NaN\n",
        "  x_del_nan_train = x_str_train.dropna(axis='columns', thresh= round(0.5 * len(x_str_train)))\n",
        "  x_del_nan_test = x_str_test.dropna(axis='columns', thresh= round(0.5 * len(x_str_test)))\n",
        "  ## Replace all NaNs with the trainings set column's median\n",
        "  x_finished_train = x_del_nan_train.fillna(x_del_nan_train.median())\n",
        "  x_finished_test = x_del_nan_test.fillna(x_del_nan_train.median())\n",
        "  \n",
        "  return x_finished_train, x_finished_test\n",
        "\n",
        "\n",
        "def standardscaler(x_train, x_test):\n",
        "  '''\n",
        "  Scale all values using standard scaling\n",
        "  '''\n",
        "  ## Design scaler\n",
        "  scaler = preprocessing.StandardScaler()\n",
        "  # scaler = preprocessing.RobustScaler(quantile_range=[5, 95])\n",
        "  scaler.fit(x_train)\n",
        "\n",
        "  ## Apply scaler to both sets and return scaled sets\n",
        "  x_scaled_train = scaler.transform(x_train)\n",
        "  x_scaled_test = scaler.transform(x_test)\n",
        "\n",
        "  x_scaled_df_train = pd.DataFrame(x_scaled_train, columns=x_train.columns)\n",
        "  x_scaled_df_test = pd.DataFrame(x_scaled_test, columns=x_test.columns)\n",
        "\n",
        "  return x_scaled_df_train, x_scaled_df_test\n",
        "\n",
        "def pair_plot(x,y,features):\n",
        "  '''\n",
        "  Plot the selected features using a pairplot\n",
        "  '''\n",
        "  x_pairplot = pd.DataFrame(x)\n",
        "  x_pairplot.columns = features\n",
        "  y_pairplot = pd.DataFrame(y, columns=['label'])\n",
        "  y_pairplot = y_pairplot.reset_index(drop=True)\n",
        "  total = pd.concat([x_pairplot, y_pairplot], axis=1)\n",
        "  # fig = seaborn.pairplot(total, hue='label')\n",
        "\n",
        "\n",
        "def print_result(result, feature, multiply):\n",
        "  '''\n",
        "  Print the result dataframe\n",
        "  Print the prevalence of each classifier in the result dataframe\n",
        "  Print the mean accuracy of the overall machine learning algorithm\n",
        "  --------------\n",
        "  print the features used in the cross validation\n",
        "  '''\n",
        "  print('Results'+'\\n'+'-'*80)\n",
        "  print(result)\n",
        "  print('='*80+'\\n'+'Results over all iterations:')\n",
        "  print('Mean AUC:',result['Area Under the Curve'].mean(), \n",
        "        '(min:', result['Area Under the Curve'].min(), 'max:', \n",
        "        result['Area Under the Curve'].max(), ')')\n",
        "  print('Sensitivity:',result['Sensitivity'].mean(),\n",
        "        '(min:', result['Sensitivity'].min(), 'max:', \n",
        "        result['Sensitivity'].max(), ')')\n",
        "  print('Specificity:',result['Specificity'].mean(), \n",
        "        '(min:', result['Specificity'].min(), 'max:', \n",
        "        result['Specificity'].max(), ')')\n",
        "  print('='*80)\n",
        "  if multiply[0].upper() == 'UNIVARIATE':\n",
        "    print('Prevalence of selected features:'+'\\n'+'-'*80+'\\n')\n",
        "    print(feature['Feature'].value_counts(normalize=True)*100*multiply[1])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZwdQJr3Qlxz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Model optimization functions\n",
        "'''\n",
        "\n",
        "\n",
        "def classif_hyperpar(x_train, y_train, feat_name):\n",
        "  '''\n",
        "  perform cross validation for multiple classifiers with different hyperparameters\n",
        "  '''\n",
        "  optim_classif = pd.DataFrame(columns=['clf_name', feat_name])\n",
        "  train_auc = []\n",
        "  classif_name = []\n",
        "\n",
        "  ## Random Forest classifier optimization\n",
        "  number_of_trees = [10, 30, 100, 200]\n",
        "  for tree in number_of_trees:\n",
        "    clf = RandomForestClassifier(n_estimators=tree)\n",
        "    scores = cross_val_score(clf, x_train, y_train, cv=5, scoring='roc_auc')\n",
        "    mean_scores = scores.mean()\n",
        "    train_auc.append(mean_scores)\n",
        "    classif_name.append(f'RF {tree}')\n",
        "    \n",
        "  ## SVM classifier optimization\n",
        "  slacks = [0.3, 0.1, 0.05]\n",
        "  kernels = ['linear', 'rbf', 'poly']\n",
        "  for kern in kernels:\n",
        "    for slack in slacks:\n",
        "      clf = svm.SVC(C= slack, kernel= kern)\n",
        "      scores = cross_val_score(clf, x_train, y_train, cv=5, scoring='roc_auc')\n",
        "      mean_scores = scores.mean()\n",
        "      train_auc.append(mean_scores)\n",
        "      classif_name.append(f'SVM {kern} {slack}')\n",
        "\n",
        "  ## k-nearest neighbors classifier optimization\n",
        "  number_of_neighbors = [3, 7, 11, 15]\n",
        "  for neighbor in number_of_neighbors:\n",
        "    clf = KNeighborsClassifier(n_neighbors= neighbor)\n",
        "    scores = cross_val_score(clf, x_train, y_train, cv=5, scoring='roc_auc')\n",
        "    mean_scores = scores.mean()\n",
        "    train_auc.append(mean_scores)\n",
        "    classif_name.append(f'KNN {neighbor}')\n",
        "\n",
        "  optim_classif['clf_name'] = classif_name\n",
        "  optim_classif[feat_name] = train_auc\n",
        "\n",
        "  return optim_classif\n",
        "\n",
        "  \n",
        "def features_hyperpar(x_train, y_train):\n",
        "  '''\n",
        "  Select discriminating features using PCA & univariate feature selection \n",
        "  '''\n",
        "  data_optim = pd.DataFrame()\n",
        "\n",
        "  ## PCA feature selection optimization\n",
        "  pca = PCA(n_components=None)\n",
        "  pca.fit(x_train)\n",
        "\n",
        "  variancelist = np.cumsum(pca.explained_variance_ratio_)\n",
        "\n",
        "  ## Thresholds used in gridsearch\n",
        "  thresholds = [0.5, 0.75, 0.9, 0.95]\n",
        "  for thres in thresholds:\n",
        "    comp = np.searchsorted(variancelist, thres)\n",
        "\n",
        "    pca_spec = PCA(n_components=comp)\n",
        "    pca_spec.fit(x_train)\n",
        "  \n",
        "    ## Use PCA for all different classifiers to be optimized\n",
        "    x_pca_train = pd.DataFrame(pca_spec.transform(x_train))\n",
        "    PCA_name = f'PCA {thres}'\n",
        "    PCA_classif = classif_hyperpar(x_pca_train, y_train, PCA_name)\n",
        "    if data_optim.empty:\n",
        "      data_optim = PCA_classif\n",
        "    elif data_optim.empty is False:\n",
        "      data_optim = data_optim.merge(PCA_classif, how='outer', on='clf_name')\n",
        "\n",
        "  ## Univariate feature selection (kbest) optimization\n",
        "  ## thresholds used in gridsearch \n",
        "  number_of_features = [5, 10, 25, 50, 100]\n",
        "  for number in number_of_features:  \n",
        "    ## Apply kbest, with given threshold, for all different classifiers to be optimized\n",
        "    Kbest = SelectKBest(f_classif, k=number).fit(x_train, y_train)\n",
        "    x_Kbest_train = pd.DataFrame(Kbest.transform(x_train))\n",
        "    \n",
        "    Kbest_name = f'Univariate {number}'\n",
        "    Kbest_classif = classif_hyperpar(x_Kbest_train, y_train, Kbest_name)\n",
        "    if data_optim.empty:\n",
        "      data_optim = Kbest_classif\n",
        "    elif data_optim.empty is False:\n",
        "      data_optim = data_optim.merge(Kbest_classif, how='outer', on='clf_name')\n",
        "\n",
        "  return data_optim\n",
        "\n",
        "\n",
        "def gridsearch (data):\n",
        "  '''\n",
        "  perform a manual gridsearch to perform model optimization\n",
        "  '''\n",
        "  data_train, data_test, labels_train, labels_test = split(data)\n",
        "  x_train, x_test = delnan(data_train, data_test)\n",
        "  x_scaled_train, x_scaled_test = standardscaler(x_train, x_test)\n",
        "  data_optim = features_hyperpar(x_scaled_train, labels_train)\n",
        "  \n",
        "  ## print and save results\n",
        "  pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "  print('\\033[1m'+'Results'+'\\033[0m')\n",
        "  print(data_optim)\n",
        "  \n",
        "  avg = data_optim.mean()\n",
        "  print('-'*80)\n",
        "  print('\\033[1m'+'Mean performance for each feature selection method:'+'\\033[0m')\n",
        "  print(f'{avg}')\n",
        "\n",
        "\n",
        "  maximum = avg.idxmax()\n",
        "  print('-'*80)\n",
        "  print('\\033[1m'+'Feature selection method with best mean performance:'+'\\033[0m')\n",
        "  print(f'{maximum}')\n",
        "\n",
        "  clf = data_optim[str(maximum)].idxmax()\n",
        "  value = data_optim[str(maximum)].max()\n",
        "  print('-'*80)\n",
        "  print('\\033[1m'+f'Classifier with best performance using {maximum}:'+'\\033[0m')\n",
        "  print(f\"{data_optim['clf_name'][clf]}: {value}\")\n",
        "  print('-'*80)\n",
        "  print('\\033[1m'+f\"Below, fill in {maximum} and {data_optim['clf_name'][clf]}\"+'\\033[0m')\n",
        "\n",
        "  ## Save results\n",
        "  path = Path('optimization_TM10007.csv')\n",
        "  data_optim.to_csv(path)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcwBJnEwRqfy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Classifier performance evaluation functions\n",
        "'''\n",
        "def select_features(x_train, x_test, y_train, parameters):\n",
        "  '''\n",
        "  Select discriminating features using PCA or univariate feature selection \n",
        "  '''\n",
        "  ## PCA feature selection\n",
        "  if parameters[0].upper() == 'PCA':\n",
        "    pca = PCA(n_components=None)\n",
        "    pca.fit(x_train)\n",
        "\n",
        "    variancelist = np.cumsum(pca.explained_variance_ratio_)\n",
        "\n",
        "    ## Determine the amount of features within the given threshold\n",
        "    comp = np.searchsorted(variancelist, parameters[1])\n",
        "\n",
        "    pca_spec = PCA(n_components=comp)\n",
        "    pca_spec.fit(x_train)\n",
        "    \n",
        "    ## Apply PCA to the different sets\n",
        "    x_feat_selected_train = pd.DataFrame(pca_spec.transform(x_train))\n",
        "    x_feat_selected_test = pd.DataFrame(pca_spec.transform(x_test))\n",
        "\n",
        "    \n",
        "    return x_feat_selected_train, x_feat_selected_test, _\n",
        "  \n",
        "  ## Select the best features and apply them to the different sets\n",
        "  elif parameters[0].upper() == 'UNIVARIATE':\n",
        "    Kbest = SelectKBest(f_classif, k=parameters[1]).fit(x_train, y_train)\n",
        "    x_feat_selected_train = pd.DataFrame(Kbest.transform(x_train))\n",
        "    x_feat_selected_test = pd.DataFrame(Kbest.transform(x_test))\n",
        "\n",
        "    ## determine the used features\n",
        "    feature_names = list(x_train.columns.values)\n",
        "    mask = Kbest.get_support() #list of booleans\n",
        "    used_features = [] # The list of the best features\n",
        "\n",
        "    for bool, feature in zip(mask, feature_names):\n",
        "      if bool:\n",
        "          used_features.append(feature)\n",
        "\n",
        "    return x_feat_selected_train, x_feat_selected_test, used_features\n",
        "\n",
        "\n",
        "def clf_test(x_train, x_test, y_train,  y_test, parameters):\n",
        "  '''\n",
        "   Evaluate classifier performance\n",
        "  '''\n",
        "  ## RF classifier\n",
        "  if parameters[2].upper() == 'RF':\n",
        "    clf = RandomForestClassifier(n_estimators=parameters[3])\n",
        "    clf.fit(x_train, y_train)\n",
        "    y_pred = clf.predict_proba(x_test)[:,1]\n",
        "    auc = roc_auc_score(y_test, y_pred)\n",
        "    y_pred_sens = clf.predict(x_test)\n",
        "    matrix = confusion_matrix(y_test, y_pred_sens)\n",
        "    sens = matrix[0,0] / (matrix[0,0]+matrix[0,1])\n",
        "    spec = matrix[1,1] / (matrix[1,0]+matrix[1,1])\n",
        "\n",
        "    perf = [auc, sens, spec]\n",
        "    return perf\n",
        "\n",
        "  ## SVM classifier\n",
        "  elif parameters[2].upper() == 'SVM':\n",
        "    clf = svm.SVC(C= parameters[4], kernel= parameters[5], probability=True)\n",
        "    clf.fit(x_train, y_train)\n",
        "    y_pred = clf.predict_proba(x_test)[:,1]\n",
        "    auc = roc_auc_score(y_test, y_pred)\n",
        "    y_pred_sens = clf.predict(x_test)\n",
        "    matrix = confusion_matrix(y_test, y_pred_sens)\n",
        "    sens = matrix[0,0] / (matrix[0,0]+matrix[0,1])\n",
        "    spec = matrix[1,1] / (matrix[1,0]+matrix[1,1])\n",
        "\n",
        "    perf = [auc, sens, spec]\n",
        "    return perf\n",
        "\n",
        "  ## KNN classifier\n",
        "  elif parameters[2].upper() == 'KNN':\n",
        "    clf = KNeighborsClassifier(n_neighbors=parameters[6])\n",
        "    clf.fit(x_train, y_train)\n",
        "    y_pred=clf.predict_proba(x_test)[:,1]\n",
        "    auc = roc_auc_score(y_test, y_pred)\n",
        "    y_pred_sens = clf.predict(x_test)\n",
        "    matrix = confusion_matrix(y_test, y_pred_sens)\n",
        "    sens = matrix[0,0] / (matrix[0,0]+matrix[0,1])\n",
        "    spec = matrix[1,1] / (matrix[1,0]+matrix[1,1])\n",
        "\n",
        "    perf = [auc, sens, spec]\n",
        "    return perf\n",
        "\n",
        "\n",
        "def test_performance(x_train, x_test, y_train, y_test, parameters):\n",
        "  '''\n",
        "  perform all steps required to test optimized model performance\n",
        "  '''\n",
        "\n",
        "  ## use given scaling method, clf and hyperparameters to test performance\n",
        "  x_train, x_test = delnan(x_train, x_test)\n",
        "  x_train, x_test = standardscaler(x_train, x_test)\n",
        "  x_train, x_test, features = select_features(x_train, x_test, y_train, \n",
        "                                             parameters)\n",
        "  # pair_plot(x_train, y_train, features)\n",
        "  perf = clf_test(x_train, x_test, y_train, y_test, parameters)\n",
        "\n",
        "  return clf, perf, features\n",
        "\n",
        "\n",
        "def start_test(data, iterations, feature_method, feature_threshold, clf, \n",
        "                trees, slack, kernel, neighbors):\n",
        "  '''\n",
        "  perform classifier evaluation 'iterations' times\n",
        "  '''\n",
        "\n",
        "  iteration = 0\n",
        "  parameters = [feature_method, feature_threshold, clf, \n",
        "                trees, slack, kernel, neighbors]\n",
        "  outcome = pd.DataFrame(columns=['Area Under the Curve', 'Sensitivity', \n",
        "                                  'Specificity'])\n",
        "  used_features = pd.DataFrame(columns=['Feature'])\n",
        "  \n",
        "  ## Run the script the desired amount of iterations\n",
        "  while iteration < iterations:\n",
        "    x_train, x_test, y_train, y_test = split(data)\n",
        "    clf, perf, features = test_performance(x_train, x_test, y_train, \n",
        "                                           y_test, parameters)\n",
        "    \n",
        "    ## add result of iteration to dataframe\n",
        "    add_result = {'Area Under the Curve': perf[0],\n",
        "                  'Sensitivity': perf[1], 'Specificity': perf[2]}\n",
        "    outcome = outcome.append(add_result, ignore_index=True)\n",
        "    \n",
        "    ## add features used in iteration to dataframe\n",
        "    for feature in features:\n",
        "      add_feature = {'Feature': feature}\n",
        "      used_features = used_features.append(add_feature, ignore_index=True)\n",
        "      \n",
        "    iteration += 1\n",
        "\n",
        "  ## Print results\n",
        "  print_result(outcome, used_features, parameters)\n",
        "\n",
        "  Features = used_features['Feature'].value_counts(normalize=True)*100*parameters[1]\n",
        "  ## Save results\n",
        "  path_results = Path('test_results_TM10007.csv')\n",
        "  path_features = Path('used_features_TM10007.csv')\n",
        "  outcome.to_csv(path_results)\n",
        "  Features.to_csv(path_features)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxWdQ7Y0mH1p",
        "colab_type": "code",
        "outputId": "c3ab5897-9c52-4e49-a241-42b7c836d211",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "'''\n",
        "run all steps required for hyperparameter optimization\n",
        "and print the dataframe containing grid search results.\n",
        "\n",
        "Use the results in the following steps\n",
        "'''\n",
        "gridsearch(data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mResults\u001b[0m\n",
            "           clf_name   PCA 0.5  PCA 0.75   PCA 0.9  PCA 0.95  Univariate 5  \\\n",
            "0             RF 10  0.848061  0.810555  0.769378  0.785374      0.934569   \n",
            "1             RF 30  0.862630  0.888974  0.840127  0.855655      0.934245   \n",
            "2            RF 100  0.871046  0.899128  0.881614  0.911601      0.960080   \n",
            "3            RF 200  0.865548  0.909064  0.886481  0.896116      0.946811   \n",
            "4    SVM linear 0.3  0.846116  0.896370  0.938362  0.917520      0.948336   \n",
            "5    SVM linear 0.1  0.847293  0.917734  0.942908  0.917520      0.948336   \n",
            "6   SVM linear 0.05  0.843429  0.911598  0.953182  0.917520      0.943523   \n",
            "7       SVM rbf 0.3  0.843904  0.883496  0.896905  0.896791      0.944566   \n",
            "8       SVM rbf 0.1  0.845267  0.885849  0.896905  0.896791      0.946076   \n",
            "9      SVM rbf 0.05  0.845267  0.885849  0.896905  0.896791      0.946043   \n",
            "10     SVM poly 0.3  0.817199  0.829860  0.823275  0.824378      0.938710   \n",
            "11     SVM poly 0.1  0.831618  0.831952  0.835100  0.834840      0.937908   \n",
            "12    SVM poly 0.05  0.839004  0.832828  0.841898  0.834472      0.935481   \n",
            "13            KNN 3  0.767169  0.848476  0.872597  0.858874      0.885374   \n",
            "14            KNN 7  0.804395  0.875608  0.891417  0.888372      0.914759   \n",
            "15           KNN 11  0.846868  0.888660  0.869856  0.870211      0.939682   \n",
            "16           KNN 15  0.846497  0.871979  0.874997  0.886932      0.942600   \n",
            "\n",
            "    Univariate 10  Univariate 25  Univariate 50  Univariate 100  \n",
            "0        0.935882       0.961033       0.952938        0.957443  \n",
            "1        0.963112       0.968135       0.958168        0.966070  \n",
            "2        0.959759       0.976547       0.967584        0.962413  \n",
            "3        0.969024       0.971925       0.966203        0.966120  \n",
            "4        0.949626       0.955628       0.933944        0.916430  \n",
            "5        0.951898       0.960468       0.941257        0.927360  \n",
            "6        0.950535       0.962667       0.945963        0.935354  \n",
            "7        0.943656       0.938015       0.936791        0.948309  \n",
            "8        0.937928       0.939191       0.938001        0.945809  \n",
            "9        0.937848       0.939191       0.938001        0.945809  \n",
            "10       0.943563       0.934960       0.928676        0.930842  \n",
            "11       0.922025       0.935989       0.928636        0.930541  \n",
            "12       0.917440       0.936070       0.926364        0.932968  \n",
            "13       0.890759       0.919566       0.935702        0.909662  \n",
            "14       0.937523       0.945134       0.935237        0.935030  \n",
            "15       0.949853       0.945023       0.936451        0.929943  \n",
            "16       0.945725       0.959191       0.930521        0.928727  \n",
            "--------------------------------------------------------------------------------\n",
            "\u001b[1mMean performance for each feature selection method:\u001b[0m\n",
            "PCA 0.5           0.839489\n",
            "PCA 0.75          0.874587\n",
            "PCA 0.9           0.877171\n",
            "PCA 0.95          0.875868\n",
            "Univariate 5      0.938065\n",
            "Univariate 10     0.941539\n",
            "Univariate 25     0.949925\n",
            "Univariate 50     0.941202\n",
            "Univariate 100    0.939343\n",
            "dtype: float64\n",
            "--------------------------------------------------------------------------------\n",
            "\u001b[1mFeature selection method with best mean performance:\u001b[0m\n",
            "Univariate 25\n",
            "--------------------------------------------------------------------------------\n",
            "\u001b[1mClassifier with best performance using Univariate 25:\u001b[0m\n",
            "RF 100: 0.9765474598930481\n",
            "--------------------------------------------------------------------------------\n",
            "\u001b[1mBelow, fill in Univariate 25 and RF 100\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stvuPA2GbmUd",
        "colab_type": "code",
        "outputId": "ec8aff15-29dc-4e13-9d83-f7ae7f0978f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        }
      },
      "source": [
        "'''\n",
        "test the optimized classifier \"iterations\" times\n",
        "\n",
        "Fill in the feature selection method, classifier and optimized hyperparamaters based on \n",
        "the gridsearch above\n",
        "\n",
        "feature_method = type of feature method: ('pca', 'univariate')\n",
        "feature_threshold = hyperparameter for specific feature selection method\n",
        "                    when using pca: (0.5, 0.75, 0.9, 0.95)\n",
        "                    when using univariate: (5, 10, 25, 50, 100)\n",
        "clf = type of classifier: ('RF', 'SVM', 'KNN')\n",
        "trees = number of trees in RF: (10, 30, 100, 200)\n",
        "slack = slack value in SVM: (0.05, 0.1, '0.3)\n",
        "kernel = kernel in SVM: ('rbf', 'poly', 'linear')\n",
        "neighbors = number of neighbors in KNN: (3, 7, 11, 15)\n",
        "\n",
        "if no value is needed, fill in 0\n",
        "'''\n",
        "feature_method = 'univariate'\n",
        "feature_threshold = 25\n",
        "clf = 'RF' \n",
        "trees = 100\n",
        "slack = 0\n",
        "kernel = 0\n",
        "neighbors = 0\n",
        "\n",
        "iterations = 100\n",
        "\n",
        "start_test(data, iterations, feature_method, feature_threshold, clf, trees, slack, kernel, neighbors)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results\n",
            "--------------------------------------------------------------------------------\n",
            "    Area Under the Curve  Sensitivity  Specificity\n",
            "0               0.996337     1.000000     0.923077\n",
            "1               0.978947     1.000000     0.842105\n",
            "2               0.996337     1.000000     0.846154\n",
            "3               0.991111     1.000000     0.888889\n",
            "4               0.933712     0.954545     0.833333\n",
            "..                   ...          ...          ...\n",
            "95              0.893939     0.863636     0.750000\n",
            "96              0.963668     1.000000     0.705882\n",
            "97              0.925000     0.875000     0.800000\n",
            "98              0.957778     0.880000     1.000000\n",
            "99              0.915020     0.956522     0.727273\n",
            "\n",
            "[100 rows x 3 columns]\n",
            "================================================================================\n",
            "Results over all iterations:\n",
            "Mean AUC: 0.9536331185944246 (min: 0.8774703557312252 max: 1.0 )\n",
            "Sensitivity: 0.9262540978091315 (min: 0.8095238095238095 max: 1.0 )\n",
            "Specificity: 0.8416513173788711 (min: 0.5454545454545454 max: 1.0 )\n",
            "================================================================================\n",
            "Prevalence of selected features:\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "VOLUME_NET_OVER_BRAIN             100.0\n",
            "SOLIDITY_NET                      100.0\n",
            "VOLUME_ET_over_TC                 100.0\n",
            "VOLUME_ET                         100.0\n",
            "TEXTURE_GLRLM_ET_FLAIR_GLV        100.0\n",
            "                                  ...  \n",
            "TEXTURE_GLCM_NET_T1_SumAverage      1.0\n",
            "TEXTURE_GLRLM_ET_FLAIR_SRLGE        1.0\n",
            "TEXTURE_GLSZM_NET_T1Gd_LZHGE        1.0\n",
            "TEXTURE_GLSZM_ET_T1Gd_SZE           1.0\n",
            "TEXTURE_GLSZM_ET_FLAIR_LGZE         1.0\n",
            "Name: Feature, Length: 79, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}