{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data_TM10007.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JSSchouten/TM10007_Group_10/blob/master/Data_TM10007.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AV8PjjK_vMDk",
        "colab_type": "code",
        "outputId": "5d1fbb93-df4d-4376-a801-1ce7ec4080cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Run this to use from colab environment\n",
        "!pip install -q --upgrade git+https://github.com/karinvangarderen/tm10007_project.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for brats (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFZRWeciabPz",
        "colab_type": "text"
      },
      "source": [
        "# Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCMSf2jf7Rut",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "all necessary imports\n",
        "'''\n",
        "# general functions\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "# load data\n",
        "from brats.load_data  import load_data\n",
        "\n",
        "# preprocessing and scaling\n",
        "from sklearn.model_selection    import train_test_split\n",
        "from sklearn                    import preprocessing\n",
        "from sklearn.decomposition      import PCA\n",
        "from sklearn.feature_selection  import SelectKBest, f_classif\n",
        "\n",
        "# classifiers\n",
        "from sklearn.model_selection        import cross_val_score, learning_curve\n",
        "from sklearn.neighbors              import KNeighborsClassifier\n",
        "from sklearn.ensemble               import RandomForestClassifier\n",
        "from sklearn                        import svm\n",
        "\n",
        "# calculate accuracy values\n",
        "from sklearn.metrics  import accuracy_score\n",
        "from sklearn.metrics  import confusion_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eq2zJcVgvCGE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Load the data from GitHub\n",
        "'''\n",
        "\n",
        "data = load_data()\n",
        "data_columns = list(set(data))\n",
        "# print(f'The number of samples: {len(data.index)}')\n",
        "# print(f'The number of columns: {len(data.columns)}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFqI_FVhd-4P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Learning curve function copied from assignment\n",
        "'''\n",
        "\n",
        "\n",
        "def plot_learning_curve(estimator, title, X, y, axes, ylim=None, cv=None,\n",
        "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
        "    \"\"\"\n",
        "    Generate 3 plots: the test and training learning curve, the training\n",
        "    samples vs fit times curve, the fit times vs score curve.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
        "        An object of that type which is cloned for each validation.\n",
        "\n",
        "    title : string\n",
        "        Title for the chart.\n",
        "\n",
        "    X : array-like, shape (n_samples, n_features)\n",
        "        Training vector, where n_samples is the number of samples and\n",
        "        n_features is the number of features.\n",
        "\n",
        "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
        "        Target relative to X for classification or regression;\n",
        "        None for unsupervised learning.\n",
        "\n",
        "    axes : array of 3 axes, optional (default=None)\n",
        "        Axes to use for plotting the curves.\n",
        "\n",
        "    ylim : tuple, shape (ymin, ymax), optional\n",
        "        Defines minimum and maximum yvalues plotted.\n",
        "\n",
        "    cv : int, cross-validation generator or an iterable, optional\n",
        "        Determines the cross-validation splitting strategy.\n",
        "        Possible inputs for cv are:\n",
        "          - None, to use the default 5-fold cross-validation,\n",
        "          - integer, to specify the number of folds.\n",
        "          - :term:`CV splitter`,\n",
        "          - An iterable yielding (train, test) splits as arrays of indices.\n",
        "\n",
        "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
        "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
        "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
        "\n",
        "        Refer :ref:`User Guide <cross_validation>` for the various\n",
        "        cross-validators that can be used here.\n",
        "\n",
        "    n_jobs : int or None, optional (default=None)\n",
        "        Number of jobs to run in parallel.\n",
        "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
        "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
        "        for more details.\n",
        "\n",
        "    train_sizes : array-like, shape (n_ticks,), dtype float or int\n",
        "        Relative or absolute numbers of training examples that will be used to\n",
        "        generate the learning curve. If the dtype is float, it is regarded as a\n",
        "        fraction of the maximum size of the training set (that is determined\n",
        "        by the selected validation method), i.e. it has to be within (0, 1].\n",
        "        Otherwise it is interpreted as absolute sizes of the training sets.\n",
        "        Note that for classification the number of samples usually have to\n",
        "        be big enough to contain at least one sample from each class.\n",
        "        (default: np.linspace(0.1, 1.0, 5))\n",
        "    \"\"\"\n",
        "\n",
        "    axes.set_title(title)\n",
        "    if ylim is not None:\n",
        "        axes.set_ylim(*ylim)\n",
        "    axes.set_xlabel(\"Training examples\")\n",
        "    axes.set_ylabel(\"Score\")\n",
        "\n",
        "    train_sizes, train_scores, test_scores  = \\\n",
        "        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n",
        "                       train_sizes=train_sizes)\n",
        "    train_scores_mean = np.mean(train_scores, axis=1)\n",
        "    train_scores_std = np.std(train_scores, axis=1)\n",
        "    test_scores_mean = np.mean(test_scores, axis=1)\n",
        "    test_scores_std = np.std(test_scores, axis=1)\n",
        "\n",
        "    # Plot learning curve\n",
        "    axes.grid()\n",
        "    axes.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "                         train_scores_mean + train_scores_std, alpha=0.1,\n",
        "                         color=\"r\")\n",
        "    axes.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
        "                         test_scores_mean + test_scores_std, alpha=0.1,\n",
        "                         color=\"g\")\n",
        "    axes.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
        "                 label=\"Training score\")\n",
        "    axes.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
        "                 label=\"Cross-validation score\")\n",
        "    axes.legend(loc=\"best\")\n",
        "\n",
        "    return plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNLlTCKZ1pyA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "All other functions used\n",
        "'''\n",
        "\n",
        "\n",
        "def split(data):\n",
        "  '''\n",
        "  Divide data in a training and test set 80% - 20%\n",
        "  '''\n",
        "  x = data.iloc[:,:-1]\n",
        "  y = data['label']\n",
        "  x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
        "\n",
        "  return x_train, x_test, y_train, y_test\n",
        "\n",
        "\n",
        "def delnan(x):\n",
        "  '''\n",
        "  Replace all values causing errors by NaN, and replace those and pre-\n",
        "  existing NaNs by the column's median\n",
        "  '''\n",
        "  data_columns = list(set(x))\n",
        "  ## Replace inf en -inf by NaN\n",
        "  x_inf = x.replace([np.inf, -np.inf], np.nan)\n",
        "  ## Replace strings by NaN\n",
        "  x_str = (x_inf.drop(data_columns, axis=1)\n",
        "             .join(x_inf[data_columns].apply(pd.to_numeric, errors='coerce')))\n",
        "  ## Delete all columns containing over 50% NaN\n",
        "  x_del_nan = x_str.dropna(axis='columns', thresh= round(0.5 * len(x_str)))\n",
        "  ## Replace all NaNs with the column's median\n",
        "  x_finished = x_del_nan.fillna(x_del_nan.median())\n",
        "\n",
        "  return x_finished\n",
        "\n",
        "\n",
        "def standardscaler(x_train, x_test):\n",
        "  '''\n",
        "  Scale all values using standard scaling\n",
        "  '''\n",
        "  ## Design scaler\n",
        "  scaler = preprocessing.StandardScaler()\n",
        "  # scaler = preprocessing.RobustScaler(quantile_range=[5, 95])\n",
        "  scaler.fit(x_train)\n",
        "\n",
        "  ## Apply scaler to both sets and return scaled sets\n",
        "  x_scaled_train = scaler.transform(x_train)\n",
        "  x_scaled_test = scaler.transform(x_test)\n",
        "\n",
        "  x_scaled_df_train = pd.DataFrame(x_scaled_train, columns=x_train.columns)\n",
        "  x_scaled_df_test = pd.DataFrame(x_scaled_test, columns=x_test.columns)\n",
        "\n",
        "  return x_scaled_df_train, x_scaled_df_test\n",
        "\n",
        "\n",
        "def select_features(x_train, x_test, y_train):\n",
        "  '''\n",
        "  Select discriminating features using PCA & univariate feature selection \n",
        "  '''\n",
        "  ## PCA feature selection\n",
        "  pca = PCA(n_components=None)\n",
        "  pca.fit(x_train)\n",
        "\n",
        "  variancelist = np.cumsum(pca.explained_variance_ratio_)\n",
        "  # plt.figure(figsize=(12,8))\n",
        "  # plt.plot(variancelist)\n",
        "  # plt.xlabel('number of components')\n",
        "  # plt.ylabel('cumulative explained variance');\n",
        "\n",
        "  ## Determine the amount of features containing 75% of the variance\n",
        "  comp = np.searchsorted(variancelist,0.75)\n",
        "\n",
        "  pca_spec = PCA(n_components=comp)\n",
        "  pca_spec.fit(x_train)\n",
        "  \n",
        "  ## Apply PCA to the different sets\n",
        "  x_pca_train = pd.DataFrame(pca_spec.transform(x_train))\n",
        "  x_pca_test = pd.DataFrame(pca_spec.transform(x_test))\n",
        "  \n",
        "  ## Select the five best features and apply them to the different sets\n",
        "  Kbest = SelectKBest(f_classif, k=5).fit(x_train, y_train)\n",
        "  x_Kbest_train = pd.DataFrame(Kbest.transform(x_train))\n",
        "  x_Kbest_test = pd.DataFrame(Kbest.transform(x_test))\n",
        "\n",
        "  ## determine the 5 used features\n",
        "  feature_names = list(x_train.columns.values)\n",
        "  mask = Kbest.get_support() #list of booleans\n",
        "  used_features = [] # The list of the 5 best features\n",
        "\n",
        "  for bool, feature in zip(mask, feature_names):\n",
        "    if bool:\n",
        "        used_features.append(feature)\n",
        "\n",
        "  return x_pca_train, x_Kbest_train, x_pca_test, x_Kbest_test, used_features\n",
        "\n",
        "\n",
        "def pair_plot(x,y,features):\n",
        "  '''\n",
        "  Plot the five selected features using a pairplot\n",
        "  '''\n",
        "  x_pairplot = pd.DataFrame(x)\n",
        "  x_pairplot.columns = features\n",
        "  y_pairplot = pd.DataFrame(y, columns=['label'])\n",
        "  y_pairplot = y_pairplot.reset_index(drop=True)\n",
        "  total = pd.concat([x_pairplot, y_pairplot], axis=1)\n",
        "  # fig = seaborn.pairplot(total, hue='label')\n",
        "\n",
        "\n",
        "def crosval (x_train, y_train, x_test, y_test):\n",
        "  '''\n",
        "   Determine classifier performance using cross-validation\n",
        "  '''\n",
        "  ## Identify the classifiers used for cross validation\n",
        "  clfs = [svm.SVC(C=0.05 ,kernel='linear'), KNeighborsClassifier(n_neighbors=9), RandomForestClassifier(n_estimators=5, random_state=42, max_depth=4)]\n",
        "  clfs_name = ['Support Vector Machine','K Nearest Neighbors','Random Forest']\n",
        "\n",
        "  ## Start cross validation\n",
        "  # num = 0\n",
        "  train_acc = []\n",
        "\n",
        "  for clf in clfs:\n",
        "    ## Plot learning curve\n",
        "    # fig = plt.figure(figsize=(24,8*len(clfs)))\n",
        "    # ax=fig.add_subplot(4, 3, num + 1)\n",
        "    # num += 1\n",
        "    # plot_learning_curve(clf,str(type(clf)), x_train, y_train, axes=ax)\n",
        "\n",
        "    ## Calculate performance\n",
        "    scores = cross_val_score(clf, x_train, y_train, cv=5)\n",
        "    mean_scores = scores.mean()\n",
        "    train_acc.append(mean_scores)\n",
        "\n",
        "  ## Determine the best performing classifier\n",
        "  max_score = max(train_acc)\n",
        "  array = np.array(train_acc)\n",
        "  idx_max = np.argmax(array)\n",
        "  clf_max = clfs[idx_max]\n",
        "  clf_max_name = clfs_name[idx_max]\n",
        "\n",
        "  ## Apply chosen classifier on test data\n",
        "  clf_max.fit(x_train, y_train)\n",
        "  y_pred=clf_max.predict(x_test)\n",
        "  accuracy = accuracy_score(y_test, y_pred)\n",
        "  matrix = confusion_matrix(y_test, y_pred)\n",
        "  sens = matrix[0,0] / (matrix[0,0]+matrix[0,1])\n",
        "  spec = matrix[1,1] / (matrix[1,0]+matrix[1,1])\n",
        "\n",
        "  perf = [accuracy, sens, spec]\n",
        "\n",
        "  return clf_max_name, perf\n",
        "\n",
        "\n",
        "def print_result(result, feature):\n",
        "  '''\n",
        "  Print the result dataframe\n",
        "  Print the prevalence of each classifier in the result dataframe\n",
        "  Print the mean accuracy of the overall machine learning algorithm\n",
        "  --------------\n",
        "  print the features used in the cross validation\n",
        "  '''\n",
        "  print('-'*80+'\\n'+'Result')\n",
        "  print(result)\n",
        "  print('-'*80+'\\n'+'Prevalence of classifiers')\n",
        "  print(result['Classifier'].value_counts(normalize=True)*100)\n",
        "  print('='*80+'\\n'+'Results over all iterations')\n",
        "  print('Mean accuracy:',result['Accuracy'].mean())\n",
        "  print('Sensitivity:',result['Sensitivity'].mean())\n",
        "  print('Specificity:',result['Specificity'].mean())\n",
        "  print('='*80)\n",
        "  print('Prevalence of selected features:'+'\\n'+'-'*80+'\\n')\n",
        "  print(feature['Feature'].value_counts(normalize=True)*100*5)\n",
        "\n",
        "\n",
        "def runscript (data_train, data_test, labels_train, labels_test):\n",
        "  '''\n",
        "  Run all the different functions needed for one iteration.\n",
        "  '''\n",
        "  x_train = delnan(data_train)\n",
        "  x_test = delnan(data_test)\n",
        "  x_scaled_train, x_scaled_test = standardscaler(x_train, x_test)\n",
        "  x_pca_train, x_Kbest_train, x_pca_test, x_Kbest_test, features = select_features(x_scaled_train, x_scaled_test, labels_train)\n",
        "  pair_plot(x_Kbest_train, labels_train, features)\n",
        "  clf, perf = crosval(x_Kbest_train, labels_train, x_Kbest_test, labels_test)\n",
        "\n",
        "  return clf, perf, features\n",
        "\n",
        "\n",
        "def start(data, iterations):\n",
        "  '''\n",
        "  Run the iterations. Print and save results.\n",
        "  '''\n",
        "  ## Run the script \"iterations\" time\n",
        "  iteration = 0\n",
        "  outcome = pd.DataFrame(columns=['Classifier', 'Accuracy','Sensitivity','Specificity'])\n",
        "  used_features = pd.DataFrame(columns=['Feature'])\n",
        "  while iteration < iterations:\n",
        "    x_train, x_test, y_train, y_test = split(data)\n",
        "    clf, perf, features = runscript(x_train, x_test, y_train, y_test)\n",
        "    \n",
        "    ## add results of iteration to dataframe\n",
        "    add_result = {'Classifier': str((clf)), 'Accuracy': perf[0],\n",
        "                  'Sensitivity': perf[1], 'Specificity': perf[2]}\n",
        "    outcome = outcome.append(add_result, ignore_index=True)\n",
        "    \n",
        "    ## add features used in iteration to dataframe\n",
        "    for feature in features:\n",
        "      add_feature = {'Feature': feature}\n",
        "      used_features = used_features.append(add_feature, ignore_index=True)\n",
        "      \n",
        "    iteration += 1\n",
        "  \n",
        "  ## Print results\n",
        "  print_result(outcome, used_features)\n",
        "  \n",
        "  ## Save results\n",
        "  path = Path('results_TM10007.csv')\n",
        "  outcome.to_csv(path)\n",
        "  \n",
        "  return outcome"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stvuPA2GbmUd",
        "colab_type": "code",
        "outputId": "4bd48c92-ec5c-4780-d6f1-e46facdbcc36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "source": [
        "'''\n",
        "Run the script\n",
        "arg1 = all the data\n",
        "arg2 = the number of desired iterations\n",
        "'''\n",
        "result = start(data, 100)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "Result\n",
            "                Classifier  Accuracy  Sensitivity  Specificity\n",
            "0      K Nearest Neighbors  0.823529     0.882353     0.764706\n",
            "1   Support Vector Machine  0.882353     0.952381     0.769231\n",
            "2            Random Forest  0.911765     1.000000     0.769231\n",
            "3   Support Vector Machine  0.911765     0.950000     0.857143\n",
            "4            Random Forest  0.794118     0.863636     0.666667\n",
            "..                     ...       ...          ...          ...\n",
            "95     K Nearest Neighbors  0.852941     0.952381     0.692308\n",
            "96     K Nearest Neighbors  0.823529     0.952381     0.615385\n",
            "97           Random Forest  0.911765     0.900000     0.928571\n",
            "98     K Nearest Neighbors  0.882353     0.952381     0.769231\n",
            "99  Support Vector Machine  0.882353     0.952381     0.769231\n",
            "\n",
            "[100 rows x 4 columns]\n",
            "--------------------------------------------------------------------------------\n",
            "Prevalence of classifiers\n",
            "K Nearest Neighbors       49.0\n",
            "Support Vector Machine    36.0\n",
            "Random Forest             15.0\n",
            "Name: Classifier, dtype: float64\n",
            "================================================================================\n",
            "Results over all iterations\n",
            "Mean accuracy: 0.8794117647058819\n",
            "Sensitivity: 0.9512242444312535\n",
            "Specificity: 0.7647149721193837\n",
            "================================================================================\n",
            "Prevalence of selected features:\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "VOLUME_ET_OVER_WT                100.0\n",
            "VOLUME_NET_over_TC               100.0\n",
            "VOLUME_ET_over_TC                100.0\n",
            "VOLUME_NET_OVER_WT                97.0\n",
            "SOLIDITY_NET                      55.0\n",
            "VOLUME_ET                         23.0\n",
            "VOLUME_NET_OVER_BRAIN             12.0\n",
            "VOLUME_ET_OVER_BRAIN              11.0\n",
            "TEXTURE_GLRLM_ET_T2_LRHGE          1.0\n",
            "TEXTURE_NGTDM_NET_T2_Busyness      1.0\n",
            "Name: Feature, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QisqVIlhjAvu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}